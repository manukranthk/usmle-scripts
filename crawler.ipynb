{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Crawler\n",
    "Crawler script designed to download directories of hospital websites to get staff email addresses. To add a new hospital to crawl extend the visitor class to implement the functions `filter_url` and `crawl_name_title_email_prorgam` to start crawling a new website.\n",
    "\n",
    "There is some trail and error involved with inspecting the DOM on the profile page to implement `crawl_name_title_email_prorgam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import html\n",
    "import re\n",
    "import sys\n",
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import requests.exceptions\n",
    "from urllib.parse import urlsplit\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "class InstiSiteVisitor(object):\n",
    "    def __init__(self, root, name, shortname):\n",
    "        '''\n",
    "        @root: the root URL to crawl.\n",
    "        @name: institution name.\n",
    "        '''\n",
    "        self.root = root\n",
    "        self.name = name\n",
    "        self.shortname = shortname\n",
    "\n",
    "    def print_info(self):\n",
    "        print('Name : ' + self.name + \n",
    "              ', root url : ' + self.root + \n",
    "              ', short name : ' + self.shortname)\n",
    "\n",
    "    def filter_url(self, link):\n",
    "        return 0\n",
    "    \n",
    "    def crawl_name_title_email_program(self, soup):\n",
    "        '''\n",
    "        @soup : beautifulsoup object that represents \n",
    "                the profile page of the doctor in the directory\n",
    "        '''\n",
    "        return ['', '', '', '']\n",
    "    \n",
    "    def name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def root(self):\n",
    "        return self.root\n",
    "    \n",
    "    def get_mailto_ref(self, soup):\n",
    "        email = \"\"\n",
    "        a = None\n",
    "        for anchor in soup.find_all(\"a\"):\n",
    "            # find emails\n",
    "            link = anchor.attrs[\"href\"] if \"href\" in anchor.attrs else ''\n",
    "            if link.find(\"ailto\") >= 0:\n",
    "                a = anchor\n",
    "                email = link.split(':')[1]\n",
    "                yield [email, a]\n",
    "    \n",
    "    def fetch_urls(self, soup):\n",
    "        return []\n",
    "                \n",
    "class BronxCareVisitor(InstiSiteVisitor):\n",
    "    def __init__(self):\n",
    "        super().__init__('https://www.bronxcare.org/our-services/pediatrics/', \n",
    "                         'BronxCare Health System', \n",
    "                         'bronxcare')\n",
    "\n",
    "    def filter_url(self, link):\n",
    "        if link.find('www.bronxcare.org/physicians/find-a-physician/detail') >= 0:\n",
    "            return 1\n",
    "        if link == 'https://www.bronxcare.org/physicians/find-a-physician/detail/':\n",
    "            return 0\n",
    "        super().filter_url(link)\n",
    "        \n",
    "    def crawl_name_title_email_program(self, soup):\n",
    "        div = soup.find('div', {'class':'description'})\n",
    "        name = ''\n",
    "        email = ''\n",
    "        title = ''\n",
    "        program = ''\n",
    "        \n",
    "        if div:\n",
    "            h3 = div.find('h3')\n",
    "            name_and_title = h3.text.split(\",\", 1)\n",
    "            name = name_and_title[0].strip()\n",
    "            title = name_and_title[1].strip()\n",
    "            \n",
    "        div = soup.find('div', {'class': 'physician-app profile'})\n",
    "        if div:\n",
    "            for [e, a] in super().get_mailto_ref(div):\n",
    "                email = a.text\n",
    "                break\n",
    "        return [name, title, email, program]\n",
    "\n",
    "class CincinattiVisitor(InstiSiteVisitor):\n",
    "    def __init__(self):\n",
    "        super().__init__('https://www.cincinnatichildrens.org/search/doctor-search?q=&start=0', \n",
    "                         'Cincinnati Children\\'s Hospital',\n",
    "                         'cincinnati')\n",
    "\n",
    "    def crawl_name_title_email_program(self, soup):\n",
    "        name = \"\"\n",
    "        title = \"\"\n",
    "        email = \"\"\n",
    "        program = \"\"\n",
    "        h1 = soup.find(\"h1\", {\"class\":\"person-name\"})\n",
    "        if h1:\n",
    "            parts = h1.text.split(\",\", 1)\n",
    "            name = parts[0]\n",
    "            if len(parts) >= 2:\n",
    "                title = parts[1]\n",
    "            for [e, a] in self.get_mailto_ref(soup):\n",
    "                email = e\n",
    "                break\n",
    "        return [name, title, email, program]\n",
    "    \n",
    "    def filter_url(self, link):\n",
    "        if link.find('https://www.cincinnatichildrens.org/bio/') >= 0:\n",
    "            return 1\n",
    "        if link.find('https://www.cincinnatichildrens.org/search/doctor-search') >= 0:\n",
    "            return 1\n",
    "        return super().filter_url(link)\n",
    "\n",
    "class UFL_HSCJ_Visitor(InstiSiteVisitor):\n",
    "    def __init__(self):\n",
    "        super().__init__('https://hscj.ufl.edu/pediatrics/faculty.aspx', \n",
    "                         'University of Florida',\n",
    "                         'ufl_hscj')\n",
    "    \n",
    "    def filter_url(self, link):\n",
    "        if link.find('https://hscj.ufl.edu/directory/bio/') >=0 :\n",
    "            return 1\n",
    "        return super().filter_url(link)\n",
    "    \n",
    "    def crawl_name_title_email_program(self, soup):\n",
    "        name = \"\"\n",
    "        title = \"\"\n",
    "        email = \"\"\n",
    "        program = \"\"\n",
    "        title_div = soup.find(\"div\", {\"class\":\"titleDiv\"})\n",
    "        if title_div:\n",
    "            splits = title_div.h1.text.split(\",\", 1)\n",
    "            name = splits[0]\n",
    "            title = splits[1]\n",
    "\n",
    "        for anchor in soup.find_all(\"a\"):\n",
    "            # find emails\n",
    "            link = anchor.attrs[\"href\"] if \"href\" in anchor.attrs else ''\n",
    "            if link.find(\"mailto\") >= 0:\n",
    "                parent_div = anchor.find_parent(\"div\")\n",
    "                if \"infoLeft\" in parent_div.attrs[\"class\"] :\n",
    "                    email = link.split(\":\")[1]\n",
    "        return [name, title, email, program]\n",
    "\n",
    "class BostonChildrenVisitor(InstiSiteVisitor):\n",
    "    def __init__(self):\n",
    "        super().__init__('http://www.childrenshospital.org/directory#sort='\\\n",
    "                         'relevancy&f:_3BD935D7-4C32-43AD-B7CA-5F366D2450F8=[Doctor]', \n",
    "                         'Boston Children\\'s Hospital',\n",
    "                         'boston_children')\n",
    "\n",
    "    def filter_url(self, link):\n",
    "        if link.find('http://www.childrenshospital.org/directory/physicians/') >= 0:\n",
    "            return 1\n",
    "        if link.find('http://www.childrenshospital.org/directory#') >= 0:\n",
    "            return 1\n",
    "        return super().filter_url(link)\n",
    "    \n",
    "    def crawl_name_title_email_program(self, soup):\n",
    "        name = \"\"\n",
    "        title = \"\"\n",
    "        email = \"\"\n",
    "        program = \"\"\n",
    "        div = soup.find(\"div\", {\"class\":\"doctor-info col-xs-12 col-sm-8\"})\n",
    "        if div:\n",
    "            print(div)\n",
    "            \n",
    "        return [name, title, email, program]\n",
    "    \n",
    "\n",
    "class StanfordVisitor(InstiSiteVisitor):\n",
    "    def __init__(self):\n",
    "        super().__init__('http://med.stanford.edu/pediatrics/faculty.html',\n",
    "                         'Stanford Medicine',\n",
    "                         'stanford_medicine')\n",
    "\n",
    "    def filter_url(self, link):\n",
    "        if link.find('https://med.stanford.edu/profiles/browse?org=school-of-medicine/pediatrics/') >= 0:\n",
    "            return 1\n",
    "        if link.find('https://med.stanford.edu/profiles/') >= 0:\n",
    "            return 1\n",
    "        return super().filter_url(link)\n",
    "    \n",
    "    def crawl_name_title_email_program(self, soup):\n",
    "        name = \"\"\n",
    "        title = \"\"\n",
    "        email = \"\"\n",
    "        program = \"\"\n",
    "        div = soup.find(\"div\", {\"class\":\"nameAndTitle\"})\n",
    "        if div:\n",
    "            name= div.h1.text.strip()\n",
    "            parts = div.h2.text.split(\",\")\n",
    "            title = parts[0].strip()\n",
    "            if len(parts) >= 2:\n",
    "                program = parts[1].strip()\n",
    "            if re.search('fellow', program, re.IGNORECASE) or \\\n",
    "                re.search('fellow', title, re.IGNORECASE) or \\\n",
    "                re.search('Affiliate', title, re.IGNORECASE) or \\\n",
    "                re.search('Research Engineer', title, re.IGNORECASE) or \\\n",
    "                re.search('Research Assistant', title, re.IGNORECASE):\n",
    "                return [\"\",\"\",\"\",\"\"]\n",
    "            \n",
    "        a = soup.find(\"a\", {\"class\":\"email\"})\n",
    "        if a:\n",
    "            email = a.text.strip()\n",
    "        else:\n",
    "            return [\"\",\"\",\"\",\"\"]\n",
    "            \n",
    "        return [name, title, email, program]\n",
    "\n",
    "class UCSFVisitor(InstiSiteVisitor):\n",
    "    def __init__(self):\n",
    "        super().__init__('https://pediatrics.ucsf.edu/pediatrics/faculty/a',\n",
    "                         'UCSF',\n",
    "                         'ucsf')\n",
    "\n",
    "    def filter_url(self, link):\n",
    "        if link.find('https://pediatrics.ucsf.edu/pediatrics/faculty/') >= 0:\n",
    "            return 1\n",
    "        if link.find('https://pediatrics.ucsf.edu/faculty/') >= 0:\n",
    "            return 1\n",
    "        return super().filter_url(link)\n",
    "    \n",
    "    def crawl_name_title_email_program(self, soup):\n",
    "        name = \"\"\n",
    "        title = \"\"\n",
    "        email = \"\"\n",
    "        program = \"\"\n",
    "\n",
    "        title_div = soup.find(\"div\", \\\n",
    "                        {\"class\":\"field field-name-field-profiles-titles field-type-text field-label-hidden\"})\n",
    "        if title_div:\n",
    "            title = title_div.div.div.text.strip()\n",
    "            wrapper = soup.find(\"div\", {\"class\":\"wrapper cf\"})\n",
    "            if wrapper and wrapper.h1:\n",
    "                name = wrapper.h1.text.strip()\n",
    "\n",
    "        email_div = soup.find(\"div\", \\\n",
    "                        {\"class\":\"field field-name-field-person-email field-type-email field-label-hidden\"})\n",
    "        if email_div:\n",
    "            email = email_div.div.div.a.text.strip()\n",
    "        print(email_div)\n",
    "        print(\"{0}, {1}, {2}, {3}\".format(name, email, title, program))\n",
    "        \n",
    "        if email == \"\":\n",
    "            return [\"\",\"\",\"\",\"\"]\n",
    "            \n",
    "        return [name, title, email, program]\n",
    "\n",
    "class FriedaVisitor(InstiSiteVisitor):\n",
    "    def __init__(self):\n",
    "        super().__init__('https://freida.ama-assn.org/Freida/#/program/3200121017',\n",
    "                         'Frieda',\n",
    "                         'frieda')\n",
    "\n",
    "    def filter_url(self, link):\n",
    "        if link.find('https://freida.ama-assn.org/Freida/#/program/') >= 0:\n",
    "            return 1\n",
    "        return super().filter_url(link)\n",
    "    \n",
    "    def fetch_urls(self, soup):\n",
    "        program_spans = soup.find_all(\"span\", {\"class\":\"program_id\"})\n",
    "        print (\"hello world\")\n",
    "        for s in program_spans:\n",
    "            print(s.text)\n",
    "\n",
    "    def crawl_name_title_email_program(self, soup):\n",
    "        name = \"\"\n",
    "        title = \"\"\n",
    "        email = \"\"\n",
    "        program = \"\"\n",
    "\n",
    "        program_director_div = soup.find(\"div\", \\\n",
    "                        {\"class\":\"program_director\"})\n",
    "        if program_director_div:\n",
    "            print(program_director_div.children[1].text)\n",
    "\n",
    "        return [name, title, email, program]\n",
    "\n",
    "class Crawler(object):\n",
    "    def __init__(self, df, insti_visitor, max_pages_to_crawl, crawler_timeout_s):\n",
    "        '''\n",
    "        @urls: a string containing the (comma separated) URLs to crawl\n",
    "        @insti_visitor: visitor object related to the institution\n",
    "        @max_pages_to_crawl: max pages after which the crawler should stop crawling\n",
    "        @crawler_timeout_s: timeout for the get requests\n",
    "        '''\n",
    "        self.df = df\n",
    "        self.insti_visitor = insti_visitor\n",
    "        self.urls = self.insti_visitor.root.split(',')\n",
    "        self.max_pages_to_crawl = max_pages_to_crawl\n",
    "        self.timeout = crawler_timeout_s\n",
    "        \n",
    "    def print_stats(self, n, qlen):\n",
    "        print(\"Crawled : {0}/{1}. Urls in queue : {2}\".format(n, self.max_pages_to_crawl, qlen))\n",
    "\n",
    "    def crawl(self):\n",
    "        '''\n",
    "        Iterate the list of URLs and request each page, then parse it and \n",
    "        print the emails we find. \n",
    "        '''\n",
    "        # a queue of urls to be crawled\n",
    "        new_urls = deque(self.urls)\n",
    "        processed_urls = set()\n",
    "        emails = set()\n",
    "        n = max_pages_to_crawl\n",
    "\n",
    "        while new_urls and n > 0:\n",
    "            n = n - 1\n",
    "            if (n % 10) == 0:\n",
    "                self.print_stats(n, len(new_urls))\n",
    "            url = new_urls.popleft()\n",
    "            print('Processing : ' + url)\n",
    "            #time.sleep(2)\n",
    "\n",
    "            parts = urlsplit(url)\n",
    "            base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "            path = url[:url.rfind('/') + 1] if '/' in parts.path else url\n",
    "            regex = r'mailto:.*.edu\">'\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url) #, params={'timeout':self.timeout})\n",
    "                print (response.text)\n",
    "            except (requests.exceptions.MissingSchema, \n",
    "                    requests.exceptions.ConnectionError,\n",
    "                    requests.exceptions.InvalidURL):\n",
    "                print('Skipping : ' + url)\n",
    "                continue\n",
    "            processed_urls.add(url)\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"lxml\") # soup from page\n",
    "            [name, title, email, program] = \\\n",
    "                self.insti_visitor.crawl_name_title_email_program(soup)\n",
    "            if name:\n",
    "                self.df = self.df.append({'Name': name,\n",
    "                                          'Title': title,\n",
    "                                          'Email': email,\n",
    "                                          'Program': program,\n",
    "                                          'Institution': self.insti_visitor.name}, \n",
    "                                         ignore_index=True)\n",
    "                continue\n",
    "\n",
    "            # Custom fetch urls\n",
    "            self.insti_visitor.fetch_urls(soup)\n",
    "            \n",
    "            # find and process all the anchors in the document\n",
    "            for anchor in soup.find_all(\"a\"):\n",
    "                # extract link url from the anchor\n",
    "                link = anchor.attrs[\"href\"] if \"href\" in anchor.attrs else ''\n",
    "                # resolve relative links\n",
    "                if link.startswith('/'):\n",
    "                    link = base_url + link\n",
    "                elif not link.startswith('http'):\n",
    "                    link = path + link\n",
    "                # add the new url to the queue if it was not enqueued nor processed yet\n",
    "                if not link in new_urls and not link in processed_urls:\n",
    "                    if self.insti_visitor.filter_url(link) == 1:\n",
    "                        new_urls.append(link)\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config setup for the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the visitor based on the insitution you want to crawl from!\n",
    "insti = FriedaVisitor()\n",
    "max_pages_to_crawl = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : https://freida.ama-assn.org/Freida/#/program/3200121017\n",
      "<!doctype html><html lang=\"en\"><head><meta charset=\"utf-8\"><title>FREIDA Residency Program Database | Medical Fellowship Database | AMA</title><meta name=\"description\" content=\"Looking for the right medical residency? Get started by searching 11,000 medical residency and fellowship programs on the AMA's FREIDA database.\"/><base href=\"./\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"><link rel=\"icon\" type=\"image/png\" href=\"favicon-32x32.png\"><script>var _dl = { life_stage: 'unknown', member_status: 'unknown', tam_id: 'unknown' };\n",
      "    var _trackAnalytics = function (o) {\n",
      "      try {\n",
      "        window._trackAnalyticsEvents = window._trackAnalyticsEvents || [];\n",
      "        window._trackAnalyticsEvents.push(o);\n",
      "      } catch (err) { }\n",
      "    };\n",
      "\n",
      "    var gaDataLayer = [_dl];</script><!-- Production Optimizely snippet --><script src=\"https://cdn.optimizely.com/js/1063362107.js\"></script><script>var hostUrl = window.location.origin;\n",
      "    // do not include analytics for localhost or DEV\n",
      "    // for IE purpose, using indexOf() instead of includes()\n",
      "    if ((hostUrl.indexOf('dev') === -1) && (hostUrl.indexOf('localhost') === -1)) {\n",
      "      (function (w, d, s, l, i) { w[l] = w[l] || []; w[l].push({ 'gtm.start': new Date().getTime(), event: 'gtm.js' }); var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src = '//www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f); })(window, document, 'script', 'gaDataLayer', 'GTM-MVM33G');\n",
      "    }</script><script async defer=\"defer\" src=\"https://maps.googleapis.com/maps/api/js?key=AIzaSyC8dXcie6TZopNVQm9tiRD5luBppv9pmu8\" type=\"text/javascript\"></script><link rel=\"stylesheet\" type=\"text/css\" media=\"screen, projection, print\" href=\"https://assets.ama-assn.org/resources/js/libs/jqueryui/1.7.3/themes/custom-theme-ama/jquery-ui.css\"><link href=\"styles.7e21a3df3847facbeff9.bundle.css\" rel=\"stylesheet\"/></head><body><!-- Google Tag Manager (noscript) --><noscript><iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-MVM33G\" height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript><!-- End Google Tag Manager (noscript) --><app-root></app-root><script type=\"text/javascript\" src=\"inline.318b50c57b4eba3d437b.bundle.js\"></script><script type=\"text/javascript\" src=\"polyfills.cff279b8d6032fd3ce16.bundle.js\"></script><script type=\"text/javascript\" src=\"main.2e1dfec7a21b823ed3d4.bundle.js\"></script></body></html>\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "output_columns=['Name', 'Title', 'Email', 'Program', 'Institution']\n",
    "df = pd.DataFrame(columns=output_columns)\n",
    "crawler = Crawler(df, insti, max_pages_to_crawl, crawler_timeout_s=5)\n",
    "df = crawler.crawl()\n",
    "df.drop_duplicates()\n",
    "emails_output_folder = os.path.join(str(Path.home()), \"Desktop\")\n",
    "emails_output = emails_output_folder + '/emails_' + insti.shortname + '.csv'\n",
    "df.to_csv(emails_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
